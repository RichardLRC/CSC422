C:\Users\Sheldon\AppData\Local\Microsoft\WindowsApps\python.exe D:/Developer/P05/app.py
Reading D:\Developer\P05\data\schools\training\no_cornell.csv ...
Reading D:\Developer\P05\data\schools\cornell.csv ...
Running PCA_KNN: cornell.csv ...
<function PCA_KNN.name at 0x0000013D535D8430> accuracy: 0.1774193548387097
Running NaiveBayes: cornell.csv ... 
<function NaiveBayes.name at 0x0000013D535D85E0> accuracy: 0.5161290322580645
Running MaximumEntropy: cornell.csv ...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.79176        0.354
             2          -1.78932        0.779
             3          -1.78690        0.780
             4          -1.78448        0.780
             5          -1.78206        0.780
             6          -1.77966        0.780
             7          -1.77727        0.780
             8          -1.77488        0.780
             9          -1.77251        0.780
            10          -1.77014        0.781
            11          -1.76778        0.781
            12          -1.76543        0.781
            13          -1.76309        0.781
            14          -1.76077        0.781
            15          -1.75845        0.782
            16          -1.75613        0.782
            17          -1.75383        0.784
            18          -1.75154        0.784
            19          -1.74925        0.784
            20          -1.74698        0.785
            21          -1.74471        0.785
            22          -1.74245        0.785
            23          -1.74020        0.785
            24          -1.73796        0.785
            25          -1.73572        0.786
            26          -1.73349        0.786
            27          -1.73128        0.786
            28          -1.72907        0.786
            29          -1.72686        0.786
            30          -1.72467        0.787
            31          -1.72248        0.787
            32          -1.72030        0.787
            33          -1.71813        0.787
            34          -1.71597        0.787
            35          -1.71381        0.787
            36          -1.71166        0.787
            37          -1.70952        0.787
            38          -1.70738        0.788
            39          -1.70526        0.788
            40          -1.70314        0.788
            41          -1.70102        0.789
            42          -1.69892        0.789
            43          -1.69682        0.790
            44          -1.69473        0.790
            45          -1.69265        0.790
            46          -1.69057        0.791
            47          -1.68850        0.791
            48          -1.68644        0.791
            49          -1.68438        0.792
            50          -1.68234        0.792
            51          -1.68029        0.792
            52          -1.67826        0.792
            53          -1.67623        0.792
            54          -1.67421        0.793
            55          -1.67220        0.793
            56          -1.67019        0.794
            57          -1.66819        0.794
            58          -1.66620        0.795
            59          -1.66421        0.795
            60          -1.66223        0.795
            61          -1.66025        0.795
            62          -1.65829        0.795
            63          -1.65633        0.795
            64          -1.65437        0.795
            65          -1.65242        0.796
            66          -1.65048        0.796
            67          -1.64855        0.796
            68          -1.64662        0.796
            69          -1.64470        0.796
            70          -1.64278        0.796
            71          -1.64087        0.796
            72          -1.63897        0.797
            73          -1.63707        0.797
            74          -1.63518        0.798
            75          -1.63330        0.798
            76          -1.63142        0.799
            77          -1.62955        0.799
            78          -1.62768        0.799
            79          -1.62582        0.799
            80          -1.62396        0.799
            81          -1.62212        0.800
            82          -1.62027        0.800
            83          -1.61844        0.800
            84          -1.61661        0.801
            85          -1.61478        0.801
            86          -1.61296        0.801
            87          -1.61115        0.802
            88          -1.60935        0.802
            89          -1.60754        0.802
            90          -1.60575        0.802
            91          -1.60396        0.802
            92          -1.60218        0.802
            93          -1.60040        0.803
            94          -1.59862        0.803
            95          -1.59686        0.803
            96          -1.59510        0.804
            97          -1.59334        0.804
            98          -1.59159        0.805
            99          -1.58984        0.805
         Final          -1.58810        0.805
  -0.022 assign==True and label is 'department'
  -0.022 professor==True and label is 'staff'
  -0.021 syllabu==True and label is 'project'
  -0.021 lectur==True and label is 'staff'
  -0.021 syllabu==True and label is 'department'
  -0.020 exam==True and label is 'project'
  -0.020 resum==True and label is 'course'
  -0.020 final==True and label is 'department'
  -0.020 fall==True and label is 'staff'
  -0.019 code==True and label is 'department'
<function MaximumEntropy.name at 0x0000013D5478F5E0> accuracy: 0.7217741935483871
Destroying dataframe...
Reading D:\Developer\P05\data\schools\training\no_texas.csv ...
Reading D:\Developer\P05\data\schools\texas.csv ...
Running PCA_KNN: texas.csv ...
<function PCA_KNN.name at 0x0000013D535D8430> accuracy: 0.1484375
Running NaiveBayes: texas.csv ... 
<function NaiveBayes.name at 0x0000013D535D85E0> accuracy: 0.578125
Running MaximumEntropy: texas.csv ...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.79176        0.350
             2          -1.78935        0.778
             3          -1.78695        0.778
             4          -1.78455        0.778
             5          -1.78217        0.779
             6          -1.77979        0.779
             7          -1.77742        0.779
             8          -1.77506        0.779
             9          -1.77271        0.779
            10          -1.77037        0.779
            11          -1.76804        0.779
            12          -1.76571        0.779
            13          -1.76340        0.779
            14          -1.76109        0.780
            15          -1.75880        0.780
            16          -1.75651        0.780
            17          -1.75423        0.780
            18          -1.75196        0.780
            19          -1.74970        0.780
            20          -1.74745        0.781
            21          -1.74520        0.781
            22          -1.74297        0.782
            23          -1.74074        0.782
            24          -1.73852        0.782
            25          -1.73631        0.782
            26          -1.73411        0.782
            27          -1.73191        0.782
            28          -1.72972        0.782
            29          -1.72754        0.782
            30          -1.72537        0.783
            31          -1.72320        0.783
            32          -1.72105        0.784
            33          -1.71890        0.784
            34          -1.71675        0.784
            35          -1.71462        0.785
            36          -1.71249        0.785
            37          -1.71037        0.785
            38          -1.70826        0.785
            39          -1.70615        0.785
            40          -1.70405        0.785
            41          -1.70196        0.786
            42          -1.69988        0.786
            43          -1.69780        0.786
            44          -1.69573        0.786
            45          -1.69367        0.786
            46          -1.69161        0.786
            47          -1.68956        0.786
            48          -1.68752        0.787
            49          -1.68549        0.787
            50          -1.68346        0.788
            51          -1.68144        0.788
            52          -1.67942        0.788
            53          -1.67741        0.788
            54          -1.67541        0.788
            55          -1.67342        0.788
            56          -1.67143        0.788
            57          -1.66945        0.789
            58          -1.66748        0.789
            59          -1.66551        0.789
            60          -1.66355        0.790
            61          -1.66159        0.790
            62          -1.65964        0.790
            63          -1.65770        0.791
            64          -1.65576        0.791
            65          -1.65383        0.791
            66          -1.65191        0.791
            67          -1.65000        0.791
            68          -1.64809        0.792
            69          -1.64618        0.792
            70          -1.64428        0.793
            71          -1.64239        0.793
            72          -1.64051        0.793
            73          -1.63863        0.794
            74          -1.63675        0.794
            75          -1.63489        0.794
            76          -1.63303        0.795
            77          -1.63117        0.795
            78          -1.62932        0.795
            79          -1.62748        0.795
            80          -1.62564        0.795
            81          -1.62381        0.795
            82          -1.62199        0.795
            83          -1.62017        0.796
            84          -1.61835        0.796
            85          -1.61655        0.796
            86          -1.61474        0.796
            87          -1.61295        0.796
            88          -1.61116        0.796
            89          -1.60937        0.797
            90          -1.60759        0.797
            91          -1.60582        0.797
            92          -1.60405        0.797
            93          -1.60229        0.797
            94          -1.60053        0.797
            95          -1.59878        0.797
            96          -1.59704        0.798
            97          -1.59530        0.798
            98          -1.59356        0.800
            99          -1.59183        0.800
         Final          -1.59011        0.800
  -0.022 assign==True and label is 'department'
  -0.021 syllabu==True and label is 'project'
  -0.020 syllabu==True and label is 'department'
  -0.020 exam==True and label is 'project'
  -0.020 resum==True and label is 'course'
  -0.020 final==True and label is 'department'
  -0.020 fall==True and label is 'staff'
  -0.019 professor==True and label is 'staff'
  -0.019 code==True and label is 'department'
  -0.019 solut==True and label is 'department'
<function MaximumEntropy.name at 0x0000013D5478F5E0> accuracy: 0.7890625
Destroying dataframe...
Reading D:\Developer\P05\data\schools\training\no_washington.csv ...
Reading D:\Developer\P05\data\schools\washington.csv ...
Running PCA_KNN: washington.csv ...
<function PCA_KNN.name at 0x0000013D535D8430> accuracy: 0.2894736842105263
Running NaiveBayes: washington.csv ... 
<function NaiveBayes.name at 0x0000013D535D85E0> accuracy: 0.47368421052631576
Running MaximumEntropy: washington.csv ...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.79176        0.356
             2          -1.78936        0.784
             3          -1.78696        0.784
             4          -1.78457        0.784
             5          -1.78219        0.784
             6          -1.77982        0.784
             7          -1.77745        0.784
             8          -1.77510        0.784
             9          -1.77275        0.785
            10          -1.77042        0.785
            11          -1.76809        0.785
            12          -1.76577        0.784
            13          -1.76346        0.785
            14          -1.76116        0.785
            15          -1.75887        0.785
            16          -1.75659        0.785
            17          -1.75432        0.785
            18          -1.75205        0.785
            19          -1.74980        0.785
            20          -1.74755        0.786
            21          -1.74531        0.786
            22          -1.74308        0.786
            23          -1.74086        0.786
            24          -1.73865        0.786
            25          -1.73644        0.786
            26          -1.73424        0.786
            27          -1.73205        0.787
            28          -1.72987        0.787
            29          -1.72769        0.787
            30          -1.72552        0.788
            31          -1.72337        0.788
            32          -1.72121        0.788
            33          -1.71907        0.788
            34          -1.71693        0.789
            35          -1.71480        0.789
            36          -1.71268        0.789
            37          -1.71056        0.789
            38          -1.70846        0.790
            39          -1.70635        0.790
            40          -1.70426        0.790
            41          -1.70217        0.790
            42          -1.70010        0.791
            43          -1.69802        0.791
            44          -1.69596        0.791
            45          -1.69390        0.791
            46          -1.69185        0.791
            47          -1.68980        0.792
            48          -1.68777        0.792
            49          -1.68574        0.793
            50          -1.68371        0.793
            51          -1.68169        0.793
            52          -1.67968        0.793
            53          -1.67768        0.793
            54          -1.67568        0.793
            55          -1.67369        0.793
            56          -1.67171        0.793
            57          -1.66973        0.793
            58          -1.66776        0.793
            59          -1.66580        0.793
            60          -1.66384        0.794
            61          -1.66189        0.794
            62          -1.65994        0.795
            63          -1.65801        0.795
            64          -1.65607        0.795
            65          -1.65415        0.796
            66          -1.65223        0.796
            67          -1.65032        0.796
            68          -1.64841        0.796
            69          -1.64651        0.797
            70          -1.64462        0.797
            71          -1.64273        0.797
            72          -1.64085        0.797
            73          -1.63897        0.797
            74          -1.63710        0.798
            75          -1.63524        0.798
            76          -1.63338        0.798
            77          -1.63153        0.798
            78          -1.62968        0.798
            79          -1.62784        0.798
            80          -1.62601        0.799
            81          -1.62418        0.799
            82          -1.62236        0.799
            83          -1.62054        0.799
            84          -1.61873        0.799
            85          -1.61693        0.800
            86          -1.61513        0.800
            87          -1.61333        0.800
            88          -1.61155        0.800
            89          -1.60976        0.801
            90          -1.60799        0.801
            91          -1.60622        0.801
            92          -1.60445        0.801
            93          -1.60269        0.801
            94          -1.60094        0.801
            95          -1.59919        0.802
            96          -1.59744        0.802
            97          -1.59571        0.803
            98          -1.59397        0.803
            99          -1.59225        0.803
         Final          -1.59053        0.803
  -0.022 assign==True and label is 'department'
  -0.022 professor==True and label is 'staff'
  -0.021 syllabu==True and label is 'project'
  -0.021 grade==True and label is 'project'
  -0.020 syllabu==True and label is 'department'
  -0.020 resum==True and label is 'course'
  -0.020 exam==True and label is 'project'
  -0.020 fall==True and label is 'staff'
  -0.019 usa==True and label is 'course'
  -0.019 california==True and label is 'staff'
<function MaximumEntropy.name at 0x0000013D5478F5E0> accuracy: 0.7293233082706767
Destroying dataframe...
Reading D:\Developer\P05\data\schools\training\no_wisconsin.csv ...
Reading D:\Developer\P05\data\schools\wisconsin.csv ...
Running PCA_KNN: wisconsin.csv ...
<function PCA_KNN.name at 0x0000013D535D8430> accuracy: 0.26479750778816197
Running NaiveBayes: wisconsin.csv ... 
<function NaiveBayes.name at 0x0000013D535D85E0> accuracy: 0.48598130841121495
Running MaximumEntropy: wisconsin.csv ...
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.79176        0.354
             2          -1.78937        0.776
             3          -1.78699        0.775
             4          -1.78461        0.776
             5          -1.78225        0.776
             6          -1.77989        0.776
             7          -1.77754        0.777
             8          -1.77520        0.777
             9          -1.77287        0.777
            10          -1.77055        0.777
            11          -1.76824        0.777
            12          -1.76593        0.778
            13          -1.76364        0.778
            14          -1.76135        0.778
            15          -1.75908        0.779
            16          -1.75681        0.779
            17          -1.75455        0.779
            18          -1.75231        0.779
            19          -1.75006        0.779
            20          -1.74783        0.779
            21          -1.74561        0.780
            22          -1.74339        0.780
            23          -1.74119        0.781
            24          -1.73899        0.781
            25          -1.73679        0.781
            26          -1.73461        0.781
            27          -1.73244        0.781
            28          -1.73027        0.781
            29          -1.72811        0.782
            30          -1.72596        0.782
            31          -1.72381        0.782
            32          -1.72167        0.782
            33          -1.71954        0.782
            34          -1.71742        0.782
            35          -1.71530        0.782
            36          -1.71320        0.783
            37          -1.71110        0.783
            38          -1.70900        0.783
            39          -1.70692        0.783
            40          -1.70484        0.783
            41          -1.70276        0.783
            42          -1.70070        0.783
            43          -1.69864        0.783
            44          -1.69659        0.783
            45          -1.69455        0.783
            46          -1.69251        0.784
            47          -1.69048        0.784
            48          -1.68845        0.784
            49          -1.68644        0.785
            50          -1.68443        0.785
            51          -1.68242        0.786
            52          -1.68043        0.786
            53          -1.67844        0.786
            54          -1.67645        0.787
            55          -1.67448        0.787
            56          -1.67251        0.787
            57          -1.67054        0.787
            58          -1.66858        0.788
            59          -1.66663        0.788
            60          -1.66469        0.788
            61          -1.66275        0.788
            62          -1.66082        0.789
            63          -1.65889        0.789
            64          -1.65698        0.789
            65          -1.65506        0.789
            66          -1.65316        0.790
            67          -1.65126        0.790
            68          -1.64936        0.791
            69          -1.64747        0.791
            70          -1.64559        0.791
            71          -1.64372        0.791
            72          -1.64185        0.791
            73          -1.63998        0.791
            74          -1.63813        0.791
            75          -1.63627        0.792
            76          -1.63443        0.792
            77          -1.63259        0.792
            78          -1.63076        0.792
            79          -1.62893        0.793
            80          -1.62711        0.793
            81          -1.62529        0.793
            82          -1.62348        0.793
            83          -1.62167        0.793
            84          -1.61988        0.793
            85          -1.61808        0.793
            86          -1.61629        0.793
            87          -1.61451        0.794
            88          -1.61274        0.795
            89          -1.61097        0.795
            90          -1.60920        0.796
            91          -1.60744        0.796
            92          -1.60569        0.796
            93          -1.60394        0.796
            94          -1.60219        0.797
            95          -1.60046        0.797
            96          -1.59872        0.797
            97          -1.59700        0.798
            98          -1.59528        0.798
            99          -1.59356        0.798
         Final          -1.59185        0.798
  -0.022 assign==True and label is 'department'
  -0.021 syllabu==True and label is 'project'
  -0.021 syllabu==True and label is 'department'
  -0.020 resum==True and label is 'course'
  -0.020 grade==True and label is 'project'
  -0.020 exam==True and label is 'project'
  -0.020 final==True and label is 'department'
  -0.020 professor==True and label is 'staff'
  -0.020 fall==True and label is 'staff'
  -0.020 code==True and label is 'department'
<function MaximumEntropy.name at 0x0000013D5478F5E0> accuracy: 0.7725856697819314
Destroying dataframe...

Process finished with exit code 0
